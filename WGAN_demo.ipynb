{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, '../')\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gantools import data\n",
    "from gantools import utils\n",
    "from gantools import plot\n",
    "from gantools.model import WGAN, CosmoWGAN\n",
    "from gantools.gansystem import GANsystem\n",
    "from gantools.data import fmap\n",
    "from gantools import evaluation\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = 64 # Resolution of the image\n",
    "try_resume = False # Try to resume previous simulation\n",
    "Mpch = 350 # Type of dataset (select 70 or 350)\n",
    "\n",
    "\n",
    "forward = fmap.stat_forward\n",
    "backward = fmap.stat_backward\n",
    "def non_lin(x):\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape =  (30, 256, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "dataset = data.load.load_nbody_dataset(nsamples=None, spix=ns, Mpch=Mpch, forward_map=forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset can return an iterator.\n",
    "it = dataset.iter(10)\n",
    "print(next(it).shape)\n",
    "del it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the data\n",
    "X = dataset.get_all_data().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the backward maps invert the forward map.\n",
    "assert(np.sum(np.abs(forward(backward(X))-X))< 5)\n",
    "# # For debugging\n",
    "# np.sum(np.abs(forward(backward(X))-X))\n",
    "# forward(backward(X))-X\n",
    "# x = np.arange(1e4)\n",
    "# plt.plot(x, backward(forward(x))-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(forward(backward(X))-X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the histogram of the pixel densities after the forward map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X, 100)\n",
    "print('min: {}'.format(np.min(X)))\n",
    "print('max: {}'.format(np.max(X)))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to free some memory\n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot 16 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plot.draw_images(dataset.get_samples(N=16),nx=4,ny=4);\n",
    "plt.title(\"Real samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters for the WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = '2D'\n",
    "\n",
    "global_path = 'saved_results'\n",
    "\n",
    "name = 'WGAN{}'.format(ns) + '_' + time_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = False\n",
    "\n",
    "md=64\n",
    "\n",
    "params_discriminator = dict()\n",
    "params_discriminator['stride'] = [1, 2, 2, 2, 2]\n",
    "params_discriminator['nfilter'] = [md, md, 2*md, 4*md, 8*md]\n",
    "params_discriminator['shape'] = [[4, 4],[4, 4],[4, 4], [4, 4], [4, 4]]\n",
    "params_discriminator['batch_norm'] = [bn, bn, bn, bn, bn ]\n",
    "params_discriminator['full'] = []\n",
    "params_discriminator['minibatch_reg'] = False\n",
    "params_discriminator['summary'] = True\n",
    "params_discriminator['data_size'] = 2\n",
    "params_discriminator['inception'] = False\n",
    "params_discriminator['spectral_norm'] = True\n",
    "\n",
    "params_generator = dict()\n",
    "params_generator['stride'] = [2, 2, 2, 2, 1]\n",
    "params_generator['latent_dim'] = 100\n",
    "params_generator['in_conv_shape'] =[4,4]\n",
    "params_generator['nfilter'] = [4*md, 2*md, md, md, 1]\n",
    "params_generator['shape'] = [[4, 4],[4, 4], [4, 4],[4, 4],[4, 4]]\n",
    "params_generator['batch_norm'] = [bn, bn, bn,bn ]\n",
    "params_generator['full'] = [4*4*4*md]\n",
    "params_generator['summary'] = True\n",
    "params_generator['non_lin'] = None\n",
    "params_generator['data_size'] = 2\n",
    "params_generator['inception'] = False\n",
    "params_generator['spectral_norm'] = True\n",
    "\n",
    "# Optimization parameters inspired from 'Self-Attention Generative Adversarial Networks'\n",
    "# - Spectral normalization GEN DISC\n",
    "# - Batch norm GEN\n",
    "# - TTUR ('GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium')\n",
    "# - ADAM  beta1=0 beta2=0.9, disc lr 0.0004, gen lr 0.0001\n",
    "# - Hinge loss\n",
    "# Parameters are similar to the ones in those papers...\n",
    "# - 'PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION'\n",
    "# - 'LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS'\n",
    "# - 'CGANS WITH PROJECTION DISCRIMINATOR'\n",
    "\n",
    "params_optimization = dict()\n",
    "params_optimization['batch_size'] = 32\n",
    "params_optimization['epoch'] = 100\n",
    "params_optimization['n_critic'] = 5\n",
    "# params_optimization['generator'] = dict()\n",
    "# params_optimization['generator']['optimizer'] = 'adam'\n",
    "# params_optimization['generator']['kwargs'] = {'beta1':0, 'beta2':0.9}\n",
    "# params_optimization['generator']['learning_rate'] = 0.0004\n",
    "# params_optimization['discriminator'] = dict()\n",
    "# params_optimization['discriminator']['optimizer'] = 'adam'\n",
    "# params_optimization['discriminator']['kwargs'] = {'beta1':0, 'beta2':0.9}\n",
    "# params_optimization['discriminator']['learning_rate'] = 0.0001\n",
    "\n",
    "# Cosmology parameters\n",
    "params_cosmology = dict()\n",
    "params_cosmology['forward_map'] = forward\n",
    "params_cosmology['backward_map'] = backward\n",
    "\n",
    "\n",
    "# all parameters\n",
    "params = dict()\n",
    "params['net'] = dict() # All the parameters for the model\n",
    "params['net']['generator'] = params_generator\n",
    "params['net']['discriminator'] = params_discriminator\n",
    "params['net']['cosmology'] = params_cosmology # Parameters for the cosmological summaries\n",
    "params['net']['prior_distribution'] = 'gaussian'\n",
    "params['net']['shape'] = [ns, ns, 1] # Shape of the image\n",
    "params['net']['loss_type'] = 'wasserstein' # loss ('hinge' or 'wasserstein')\n",
    "params['net']['gamma_gp'] = 10 # Gradient penalty\n",
    "\n",
    "params['optimization'] = params_optimization\n",
    "params['summary_every'] = 100 # Tensorboard summaries every ** iterations\n",
    "params['print_every'] = 50 # Console summaries every ** iterations\n",
    "params['save_every'] = 1000 # Save the model every ** iterations\n",
    "params['summary_dir'] = os.path.join(global_path, name +'_summary/')\n",
    "params['save_dir'] = os.path.join(global_path, name + '_checkpoints/')\n",
    "params['Nstats'] = 2000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume, params = utils.test_resume(try_resume, params)\n",
    "# If a model is reloaded and some parameters have to be changed, then it should be done here.\n",
    "# For example, setting the number of epoch to 5 would be:\n",
    "params['optimization']['epoch'] = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator \n",
      "--------------------------------------------------\n",
      "     The input is of size (?, 100)\n",
      "     0 Full layer with 4096 outputs\n",
      "         Size of the variables: (?, 4096)\n",
      "     Reshape to (?, 4, 4, 256)\n",
      "     1 Deconv layer with 256 channels\n",
      "         Non linearity applied\n",
      "         Size of the variables: (?, 8, 8, 256)\n",
      "     2 Deconv layer with 128 channels\n",
      "         Non linearity applied\n",
      "         Size of the variables: (?, 16, 16, 128)\n",
      "     3 Deconv layer with 64 channels\n",
      "         Non linearity applied\n",
      "         Size of the variables: (?, 32, 32, 64)\n",
      "     4 Deconv layer with 64 channels\n",
      "         Non linearity applied\n",
      "         Size of the variables: (?, 64, 64, 64)\n",
      "     5 Deconv layer with 1 channels\n",
      "         Size of the variables: (?, 64, 64, 1)\n",
      "     The output is of size (?, 64, 64, 1)\n",
      "--------------------------------------------------\n",
      "\n",
      "Discriminator \n",
      "--------------------------------------------------\n",
      "     The input is of size (?, 64, 64, 1)\n",
      "     0 Conv layer with 64 channels\n",
      "         Size of the variables: (?, 64, 64, 64)\n",
      "     1 Conv layer with 64 channels\n",
      "         Size of the variables: (?, 32, 32, 64)\n",
      "     2 Conv layer with 128 channels\n",
      "         Size of the variables: (?, 16, 16, 128)\n",
      "     3 Conv layer with 256 channels\n",
      "         Size of the variables: (?, 8, 8, 256)\n",
      "     4 Conv layer with 512 channels\n",
      "         Size of the variables: (?, 4, 4, 512)\n",
      "     Reshape to (?, 8192)\n",
      "     5 Full layer with 1 outputs\n",
      "     The output is of size (?, 1)\n",
      "--------------------------------------------------\n",
      "\n",
      " Wasserstein loss with gamma_gp=10\n",
      " Using gradients penalty\n",
      "Add summary for descriptives/mean_real\n",
      "Add summary for descriptives/var_real\n",
      "Add summary for descriptives/min_real\n",
      "Add summary for descriptives/max_real\n",
      "Add summary for descriptives/kurtosis_real\n",
      "Add summary for descriptives/skewness_real\n",
      "Add summary for descriptives/median_real\n",
      "Add summary for descriptives/mean_fake\n",
      "Add summary for descriptives/var_fake\n",
      "Add summary for descriptives/min_fake\n",
      "Add summary for descriptives/max_fake\n",
      "Add summary for descriptives/kurtosis_fake\n",
      "Add summary for descriptives/skewness_fake\n",
      "Add summary for descriptives/median_fake\n",
      "Add summary for descriptives/mean_l2\n",
      "Add summary for descriptives/var_l2\n",
      "Add summary for descriptives/min_l2\n",
      "Add summary for descriptives/max_l2\n",
      "Add summary for descriptives/kurtosis_l2\n",
      "Add summary for descriptives/skewness_l2\n",
      "Add summary for descriptives/median_l2\n",
      "Add summary for cosmology/mass_histogram_l2log\n",
      "Add summary for cosmology/peak_histogram_l2log\n",
      "Add summary for cosmology/psd_l2log\n",
      "Add summary for cosmology/global_score\n",
      "\n",
      "Build the optimizers: \n",
      " * discriminator \n",
      "kwargs: {}\n",
      "learning_rate: 3.0e-05\n",
      "optimizer: rmsprop\n",
      "\n",
      " * generator \n",
      "kwargs: {}\n",
      "learning_rate: 3.0e-05\n",
      "optimizer: rmsprop\n",
      "\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "generator/0_full/Matrix:0 (float32_ref 100x4096) [409600, bytes: 1638400]\n",
      "generator/0_full/bias:0 (float32_ref 4096) [4096, bytes: 16384]\n",
      "generator/0_deconv_2d/w:0 (float32_ref 4x4x256x256) [1048576, bytes: 4194304]\n",
      "generator/0_deconv_2d/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "generator/1_deconv_2d/w:0 (float32_ref 4x4x128x256) [524288, bytes: 2097152]\n",
      "generator/1_deconv_2d/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/2_deconv_2d/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]\n",
      "generator/2_deconv_2d/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/3_deconv_2d/w:0 (float32_ref 4x4x64x64) [65536, bytes: 262144]\n",
      "generator/3_deconv_2d/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/4_deconv_2d/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]\n",
      "generator/4_deconv_2d/biases:0 (float32_ref 1) [1, bytes: 4]\n",
      "discriminator/0_conv/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]\n",
      "discriminator/0_conv/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "discriminator/1_conv/w:0 (float32_ref 4x4x64x64) [65536, bytes: 262144]\n",
      "discriminator/1_conv/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "discriminator/2_conv/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]\n",
      "discriminator/2_conv/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/3_conv/w:0 (float32_ref 4x4x128x256) [524288, bytes: 2097152]\n",
      "discriminator/3_conv/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "discriminator/4_conv/w:0 (float32_ref 4x4x256x512) [2097152, bytes: 8388608]\n",
      "discriminator/4_conv/biases:0 (float32_ref 512) [512, bytes: 2048]\n",
      "discriminator/out/Matrix:0 (float32_ref 8192x1) [8192, bytes: 32768]\n",
      "discriminator/out/bias:0 (float32_ref 1) [1, bytes: 4]\n",
      "Total size of variables: 5012994\n",
      "Total bytes of variables: 20051976\n"
     ]
    }
   ],
   "source": [
    "wgan = GANsystem(CosmoWGAN, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.train(dataset, resume=resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new samples\n",
    "To have meaningful statistics, be sure to generate enough samples\n",
    "* 2000 : 32 x 32\n",
    "* 500 : 64 x 64\n",
    "* 200 : 128 x 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2000 # Number of samples\n",
    "gen_sample = np.squeeze(wgan.generate(N=N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a few fake samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plot.draw_images(gen_sample,nx=4,ny=4);\n",
    "plt.title(\"Fake samples\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the sample quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before computing the statistics, we need to invert the mapping\n",
    "raw_images = backward(dataset.get_samples(dataset.N))\n",
    "gen_sample_raw = backward(gen_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logel2, l2, logel1, l1 = evaluation.compute_and_plot_psd(raw_images, gen_sample_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logel2, l2, logel1, l1 = evaluation.compute_and_plot_peak_cout(raw_images, gen_sample_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logel2, l2, logel1, l1 = evaluation.compute_and_plot_mass_hist(raw_images, gen_sample_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
